{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _import modules & set constants:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enable in-line MatPlotLib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import Python modules\n",
    "from __future__ import division, print_function\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hack for live printing in iPython Notebook, adapted from:\n",
    "# http://stackoverflow.com/questions/29772158/make-ipython-notebook-print-in-real-time\n",
    "class flushfile():\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __getattr__(self,name): \n",
    "        return object.__getattribute__(self.f, name)\n",
    "    def write(self, x):\n",
    "        self.f.write(x)\n",
    "        self.f.flush()\n",
    "    def flush(self):\n",
    "        self.f.flush()\n",
    "        \n",
    "oldsysstdout = sys.stdout        \n",
    "sys.stdout = flushfile(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set CONSTANTS\n",
    "\n",
    "# using AWS EMR?\n",
    "AWS_EMR_MODE = os.path.expanduser('~') == '/home/hadoop'\n",
    "\n",
    "# data paths\n",
    "DATA_FOLDER_NAME = 'DATA___MovieLens___20M'\n",
    "DATA_REPO_URL = 'https://github.com/ChicagoBoothML/%s' % DATA_FOLDER_NAME\n",
    "MOVIES_FILE_NAME = 'movies.csv'\n",
    "RATINGS_FILE_NAMES = \\\n",
    "    ['ratings01.csv',\n",
    "     'ratings02.csv',\n",
    "     'ratings03.csv',\n",
    "     'ratings04.csv',\n",
    "     'ratings05.csv',\n",
    "     'ratings06.csv',\n",
    "     'ratings07.csv',\n",
    "     'ratings08.csv',\n",
    "     'ratings09.csv',\n",
    "     'ratings10.csv']\n",
    "\n",
    "# number of examples to display for a data set\n",
    "NB_EXAMPLES_TO_SHOW = 9\n",
    "\n",
    "# random_seed\n",
    "RANDOM_SEED = 99\n",
    "\n",
    "# Apache Spark settings\n",
    "if AWS_EMR_MODE:\n",
    "    SPARK_MODE = 'yarn-client'                 # running Spark on AWS EMR YARN cluster\n",
    "    SPARK_HOME = '/usr/lib/spark'              # default Spark installation folder on AWS EMR master node\n",
    "    SPARK_DRIVER_MEMORY = '9g'                 # memory allocated to MapReduce driver process\n",
    "    SPARK_EXECUTOR_MEMORY = '3g'               # memory allocated to each MapReduce executor process\n",
    "    SPARK_DRIVER_MAX_RESULT_SIZE = '6g'        # maximum size of objects collected back to MapReduce driver process\n",
    "else:\n",
    "    SPARK_MODE = 'local'                       # running Spark on single machine\n",
    "    SPARK_HOME = '/Applications/spark-1.5.2'   # Spark installation folder on my machine\n",
    "    SPARK_DRIVER_MEMORY = '5g'                 # memory allocated to MapReduce driver process \n",
    "    SPARK_EXECUTOR_MEMORY = '1g'               # memory allocated to each MapReduce executor process\n",
    "    SPARK_DRIVER_MAX_RESULT_SIZE = '3g'        # maximum size of objects collected back to MapReduce driver process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install ChicagoBoothML_Helpy\n",
    "CHICAGOBOOTHML_HELPY_INSTALLATION_COMMAND = \\\n",
    "    'pip install --upgrade git+git://GitHub.com/ChicagoBoothML/Helpy --no-dependencies'\n",
    "if AWS_EMR_MODE:\n",
    "    os.system('sudo %s' % CHICAGOBOOTHML_HELPY_INSTALLATION_COMMAND)\n",
    "else:\n",
    "    os.system(CHICAGOBOOTHML_HELPY_INSTALLATION_COMMAND)\n",
    "\n",
    "# import from package\n",
    "from ChicagoBoothML_Helpy.Print import printflush"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Launch PySpark and set up SparkContext & HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext: <pyspark.context.SparkContext object at 0x7f386cdee3d0>\n",
      "HiveContext: <pyspark.sql.context.HiveContext object at 0x7f386ce04a50>\n"
     ]
    }
   ],
   "source": [
    "if 'pyspark' not in vars():   # set up Apache Spark environment if not yet done so\n",
    "    \n",
    "    # set environment variables for Spark\n",
    "    os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "    os.environ['SPARK_HIVE'] = 'true'\n",
    "    \n",
    "    # enable importing of PySpark through FindSpark package\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    \n",
    "    # import PySpark and set up SparkContext (\"sc\") & HiveContext (\"hc\")\n",
    "    import pyspark\n",
    "    \n",
    "    sc = pyspark.SparkContext(\n",
    "        conf=pyspark.SparkConf()\n",
    "            .setMaster(SPARK_MODE)\n",
    "            .setAppName('BostonHousing')\n",
    "            .set('spark.driver.memory', SPARK_DRIVER_MEMORY)\n",
    "            .set('spark.executor.memory', SPARK_EXECUTOR_MEMORY)\n",
    "            .set('spark.driver.maxResultSize', SPARK_DRIVER_MAX_RESULT_SIZE))\n",
    "    \n",
    "    hc = pyspark.sql.HiveContext(sc)\n",
    "    \n",
    "print('SparkContext:', sc)\n",
    "print('HiveContext:', hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports from PySpark\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download PySpark_CSV.py and put it into SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5493  100  5493    0     0  30641      0 --:--:-- --:--:-- --:--:-- 30687\n"
     ]
    }
   ],
   "source": [
    "# download PySpark_CSV.py and put it into SparkContext\n",
    "!curl https://raw.githubusercontent.com/seahboonsiew/pyspark-csv/master/pyspark_csv.py --output pyspark_csv.py\n",
    "\n",
    "if AWS_EMR_MODE:\n",
    "    sc.addPyFile('pyspark_csv.py')\n",
    "\n",
    "from pyspark_csv import csvToDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, parse & preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning Data Repo... done!\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "print('Cloning Data Repo... ', end='')\n",
    "os.system('git clone %s' % DATA_REPO_URL)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Data Files: movies.csv, ratings01.csv, ratings02.csv, ratings03.csv, ratings04.csv, ratings05.csv, ratings06.csv, ratings07.csv, ratings08.csv, ratings09.csv, ratings10.csv, done!\n"
     ]
    }
   ],
   "source": [
    "# move data to same folder or into HDFS\n",
    "print('Moving Data Files:', end='')\n",
    "for file_name in [MOVIES_FILE_NAME] + RATINGS_FILE_NAMES:\n",
    "    print(' %s,' % file_name, end='')\n",
    "    if AWS_EMR_MODE:\n",
    "        os.system('hadoop fs -put %s %s'\n",
    "                  % (os.path.join(DATA_FOLDER_NAME, file_name), file_name))\n",
    "    elif sys.platform.startswith('win'):\n",
    "        os.system('copy /y %s %s'\n",
    "                  % (os.path.join(DATA_FOLDER_NAME, file_name), file_name))\n",
    "    else:\n",
    "        os.system('yes | cp -rf %s %s'\n",
    "                  % (os.path.join(DATA_FOLDER_NAME, file_name), file_name))\n",
    "print(' done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing movies.csv ..."
     ]
    }
   ],
   "source": [
    "print('Parsing %s...' % MOVIES_FILE_NAME, end='')\n",
    "movies_ddf = \\\n",
    "    csvToDataFrame(\n",
    "        sqlCtx=hc,\n",
    "        rdd=sc.textFile(MOVIES_FILE_NAME),\n",
    "        columns=None,\n",
    "        sep=',',\n",
    "        parseDate=True)\\\n",
    "    .cache()\n",
    "    \n",
    "movies_ddf.registerTempTable('movies')\n",
    "\n",
    "print(' done!\\n')\n",
    "movies_ddf.show(NB_EXAMPLES_TO_SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|      2|   3.5|1112486027|\n",
      "|     1|     29|   3.5|1112484676|\n",
      "|     1|     32|   3.5|1112484819|\n",
      "|     1|     47|   3.5|1112484727|\n",
      "|     1|     50|   3.5|1112484580|\n",
      "|     1|    112|   3.5|1094785740|\n",
      "|     1|    151|   4.0|1094785734|\n",
      "|     1|    223|   4.0|1112485573|\n",
      "|     1|    253|   4.0|1112484940|\n",
      "+------+-------+------+----------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Parsing ')\n",
    "for i, ratings_file_name in enumerate(RATINGS_FILE_NAMES):\n",
    "    d = csvToDataFrame(\n",
    "        sqlCtx=hc,\n",
    "        rdd=sc.textFile(ratings_file_name),\n",
    "        columns=None,\n",
    "        sep=',',\n",
    "        parseDate=True)\n",
    "    if not i:\n",
    "        ratings_ddf = d\n",
    "    else:\n",
    "        ratings_ddf = ratings_ddf.unionAll(d)\n",
    "\n",
    "ratings_ddf.cache()\n",
    "ratings_ddf.registerTempTable('ratings')\n",
    "\n",
    "ratings_ddf.show(NB_EXAMPLES_TO_SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000263"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of ratings\n",
    "ratings_ddf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _**NOTE**: the below parameters run successfully on an AWS EMR cluster of 1 + 5 nodes of type M3.xlarge_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split Train & Test sets:\n",
    "ratings_train_ddf, ratings_test_ddf = \\\n",
    "    ratings_ddf.randomSplit(\n",
    "        weights=[.5, .5],\n",
    "        seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build model pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_factor_reccommender = \\\n",
    "    ALS(\n",
    "        rank=30,\n",
    "        maxIter=30,\n",
    "        regParam=1e-3,\n",
    "        numUserBlocks=10,\n",
    "        numItemBlocks=10,\n",
    "        implicitPrefs=False,\n",
    "        alpha=1.,   # only relevant for implicit preferences\n",
    "        userCol='userId',\n",
    "        itemCol='movieId',\n",
    "        seed=RANDOM_SEED,\n",
    "        ratingCol='rating',\n",
    "        nonnegative=True,\n",
    "        checkpointInterval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_factor_rec_model = \\\n",
    "    latent_factor_reccommender.fit(\n",
    "        dataset=ratings_train_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Make & evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|    32|     31|   3.0| 845962944| 2.8142872|\n",
      "|  6632|     31|   3.5|1423926132| 2.4679818|\n",
      "|  8232|     31|   4.0| 839840269|  4.621995|\n",
      "|  9032|     31|   3.0| 934442206| 2.9327242|\n",
      "| 10632|     31|   3.0|1112458785| 4.1600375|\n",
      "| 23832|     31|   3.0|1094565708|  3.680944|\n",
      "| 32432|     31|   4.0| 844687388|  3.444585|\n",
      "| 39232|     31|   5.0| 832587925|  4.792559|\n",
      "| 39432|     31|   3.5|1214953387|  3.760844|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_ratings_ddf = \\\n",
    "    latent_factor_rec_model.transform(\n",
    "        dataset=ratings_test_ddf)\n",
    "    \n",
    "predicted_ratings_ddf.registerTempTable('predicted_ratings')\n",
    "\n",
    "predicted_ratings_ddf = hc.sql(\n",
    "    \"SELECT \\\n",
    "        * \\\n",
    "    FROM \\\n",
    "        predicted_ratings \\\n",
    "    WHERE \\\n",
    "        prediction != 'NaN'\")\\\n",
    "    .cache()\n",
    "\n",
    "predicted_ratings_ddf.cache()\n",
    "predicted_ratings_ddf.registerTempTable('predicted_ratings')\n",
    "\n",
    "predicted_ratings_ddf.show(NB_EXAMPLES_TO_SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.3768310546875, 21.41329574584961)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min & max ratings for sanity-checking\n",
    "preds = \\\n",
    "    predicted_ratings_ddf\\\n",
    "    .select('prediction')\\\n",
    "    .rdd\\\n",
    "    .map(lambda row: row[0])\\\n",
    "    .collect()\n",
    "\n",
    "# there are extreme ratings way out of bound -\n",
    "# so the recommender is not that great yet\n",
    "min(p), max(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91489142509378918"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE evaluation of bounded predictions\n",
    "numpy.sqrt(\n",
    "    hc.sql(\n",
    "        \"SELECT \\\n",
    "            SUM(POW( \\\n",
    "                (CASE \\\n",
    "                    WHEN prediction < 0.0 THEN 0.0 \\\n",
    "                    WHEN prediction > 5.0 THEN 5.0 \\\n",
    "                    ELSE prediction \\\n",
    "                    END) - rating, 2)) \\\n",
    "        FROM \\\n",
    "            predicted_ratings\")\\\n",
    "    .rdd\\\n",
    "    .map(lambda row: row[0])\\\n",
    "    .take(1)[0] / \\\n",
    "    predicted_ratings_ddf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _END!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
